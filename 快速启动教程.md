# 🚀 Jetson Benchmark 快速启动教程

## 📊 测试规模说明

### 测试次数计算
```
总推理次数 = 问题数 × Prompt级别 × 模型数
           = 80 × 5 × 4
           = 1,600 次推理
```

**详细说明：**
- **80个问题** - 来自 `question.jsonl`，涵盖8个类别
  - writing（写作）: 10题
  - roleplay（角色扮演）: 10题
  - reasoning（推理）: 10题
  - math（数学）: 10题
  - coding（编程）: 10题
  - extraction（信息提取）: 10题
  - stem（科学技术）: 10题
  - humanities（人文）: 10题

- **5个Prompt级别** - 每个问题生成5个版本 -来自'generated_prompts.xlsx'通过`question.jsonl`生成得到
  - L0: 基础版（原始问题）
  - L1: 澄清版（+约50-150 tokens）
  - L2: 引导版（+约100-200 tokens）
  - L3: 示例版（+约150-250 tokens）
  - P: 冗余版（+约100-200 tokens无效内容）

- **4个小语言模型（SLM）**
  - `qwen1.5-1.8b` - 1.8B参数，~1.2GB
  - `gemma-2-2b` - 2B参数，~1.6GB
  - `phi-3.5-mini` - 3.8B参数，~2.3GB
  - `qwen3-4b` - 4B参数，~2.6GB

---

## 📈 测量指标

每次推理会测量以下6个核心指标：

### 1. **延迟（Latency）**
- **单位**: 毫秒（ms）
- **含义**: 从输入到生成完整输出的时间
- **用途**: 评估推理速度，用户体验
- **典型值**: 500-5000ms

### 2. **能耗（Energy）**
- **单位**: 焦耳（J）
- **含义**: 单次推理消耗的电能（去除待机功耗）
- **用途**: 评估边缘设备功耗，计算运行成本
- **典型值**: 1-10J

### 3. **输入Token数（Prompt Tokens）**
- **单位**: tokens
- **含义**: 输入文本的token数量
- **用途**: 验证Prompt长度递增（L0 < L1 < L2 < L3 < P）
- **典型值**: L0=20-50, L1=70-150, L2=150-300, L3=300-500, P=400-600

### 4. **输出Token数（Completion Tokens）**
- **单位**: tokens
- **含义**: 模型生成的token数量
- **用途**: 分析Prompt长度对输出长度的影响
- **典型值**: 50-500

### 5. **质量评分（Quality Score）**
- **单位**: 0-10分
- **含义**: GPT-4评判的输出质量（4个维度总分）
  - Factuality（准确性）: 0-2.5分
  - Helpfulness（有用性）: 0-2.5分
  - Structure（结构性）: 0-2.5分
  - Conciseness（简洁性）: 0-2.5分
- **用途**: 评估Prompt长度对输出质量的影响
- **典型值**: 5-9分

### 6. **输出文本（Output Text）**
- **格式**: 纯文本字符串
- **含义**: 模型生成的完整回答
- **用途**: 人工复查，案例分析
- **保存位置**: JSONL和导出的CSV/TXT文件

---

## 🎯 核心研究问题

通过这1600次测试，我们想回答：

1. **更长的Prompt是否带来更好的质量？**
   - L3（示例引导）vs L0（基础）：质量提升了多少？

2. **质量提升的代价是什么？**
   - 延迟增加了多少？
   - 能耗增加了多少？

3. **哪个Prompt级别性价比最高？**
   - 质量/能耗比
   - 质量/延迟比

4. **不同模型的表现差异？**
   - 小模型（1.8B）vs 大模型（4B）
   - 哪个模型最适合边缘部署？

---

## ⚙️ 准备工作（在PC上）

### 步骤1：转换Prompt为CSV

```bash
# 进入项目目录
cd C:\Users\Administrator\Desktop\project\serverlessSLM

# 将Excel转换为CSV（按类别分割）
python convert_excel_to_csv.py \
  --excel generated_prompts.xlsx \
  --outdir data \
  --split
```

**输出结果：**
```
data/
├── writing_prompts_hierarchical.csv
├── roleplay_prompts_hierarchical.csv
├── reasoning_prompts_hierarchical.csv
├── math_prompts_hierarchical.csv
├── coding_prompts_hierarchical.csv
├── extraction_prompts_hierarchical.csv
├── stem_prompts_hierarchical.csv
└── humanities_prompts_hierarchical.csv
```

---

### 步骤2：下载4个GGUF模型

**需要下载的模型（记住务必Q6_K量化，固定对齐这个量化版本，如下下载链接可能有问题）：**

```bash
# 创建模型目录
mkdir models

# 模型1: Qwen 1.5 1.8B (~1.2GB)
# 下载链接：https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF
# 文件名：qwen1_5-1_8b-chat-q6_k.gguf

# 模型2: Gemma 2 2B (~1.6GB)
# 下载链接：https://huggingface.co/lmstudio-community/gemma-2-2b-it-GGUF
# 文件名：gemma-2-2b-it-Q6_K.gguf

# 模型3: Phi 3.5 Mini (~2.3GB)
# 下载链接：https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF
# 文件名：Phi-3.5-mini-instruct-Q6_K.gguf

# 模型4: Qwen 3 4B (~2.6GB)
# 下载链接：https://huggingface.co/Qwen/Qwen3-4B-GGUF
# 文件名：Qwen3-4B-Q6_K.gguf
```

**使用huggingface-cli下载（推荐）：**
```bash
# 安装工具
pip install huggingface-hub

# 下载模型
huggingface-cli download Qwen/Qwen1.5-1.8B-Chat-GGUF qwen1_5-1_8b-chat-q6_k.gguf --local-dir models/
huggingface-cli download lmstudio-community/gemma-2-2b-it-GGUF gemma-2-2b-it-Q6_K.gguf --local-dir models/
huggingface-cli download bartowski/Phi-3.5-mini-instruct-GGUF Phi-3.5-mini-instruct-Q6_K.gguf --local-dir models/
huggingface-cli download Qwen/Qwen3-4B-GGUF Qwen3-4B-Q6_K.gguf --local-dir models/
```

**验证下载：**
```bash
# 检查模型文件
ls -lh models/*.gguf

# 应该看到4个文件，总大小约7-8GB
```

---

### 步骤3：打包传输到Jetson

```bash
# 打包所有文件
tar -czf jetson_benchmark.tar.gz \
  *.py \
  *.sh \
  *.md \
  question.jsonl \
  requirements.txt \
  data/ \
  models/

# 查看压缩包大小（约8GB）
ls -lh jetson_benchmark.tar.gz

# 传输到Jetson（替换为你的Jetson IP）
scp jetson_benchmark.tar.gz jetson@192.168.x.x:~/
```

---

## 🖥️ 在Jetson上运行

### 步骤1：解压和环境设置

```bash
# SSH登录到Jetson
ssh jetson@192.168.x.x

# 解压文件
cd ~
tar -xzf jetson_benchmark.tar.gz
cd jetson_benchmark_package

# 设置Jetson为最大性能模式（重要！）
sudo nvpmodel -m 0        # 模式0 = 最大性能
sudo jetson_clocks         # 锁定最高频率

# 验证设置
sudo nvpmodel -q           # 应该显示 NV Power Mode: MAXN

# 安装Python依赖
pip3 install -r requirements.txt
```

**依赖说明：**
- `llama-cpp-python` - GGUF模型推理引擎
- `tiktoken` - Token计数工具
- `openpyxl` - Excel文件处理
- `pandas` - 数据处理
- `requests` - API调用

---

### 步骤2：运行测试（3种方式）

#### ⭐⭐⭐⭐⭐方式A：自动化脚本（墙裂这个推荐，最简单，但是注意可能会有依赖和原先python版本冲突的问题）⭐⭐⭐⭐⭐

```bash
# 交互式菜单
bash run_full_benchmark.sh

# 选择：
# 1) Full benchmark (inference + judging + aggregation)  ← 选这个
# 然后按提示操作
```

**自动化脚本会：**
1. ✅ 检查环境（tegrastats、依赖、模型）
2. ✅ 设置性能模式
3. ✅ 测量待机功耗
4. ✅ 运行1600次推理（4模型 × 5级别 × 80问题）
5. ✅ 自动导出responses为CSV和文本
6. ✅ 调用GPT-4评分（需要联网）
7. ✅ 生成Excel分析报告

---

#### 方式B：命令行一键运行

```bash
# 运行单个类别（测试用）
bash run_full_benchmark.sh full writing

# 运行所有类别（完整测试）
bash run_full_benchmark.sh full all
```

---

#### 方式C：手动分步运行（最灵活）

**第1步：运行推理（在Jetson上）**

```bash
# 单个类别测试（推荐先测试一个类别）
python3 run_llamacpp_collect.py \
  --models \
    models/qwen1_5-1_8b-chat-q6_k.gguf \
    models/gemma-2-2b-it-Q6_K.gguf \
    models/Phi-3.5-mini-instruct-Q6_K.gguf \
    models/Qwen3-4B-Q6_K.gguf \
  --prompts data/writing_prompts_hierarchical.csv \
  --out_jsonl results/runs_llamacpp.jsonl \
  --temperature 0.0 \
  --max_tokens 1024 \
  --n_ctx 4096

# 或使用通配符简化（4个模型）
python3 run_llamacpp_collect.py \
  --models models/*.gguf \
  --prompts data/writing_prompts_hierarchical.csv \
  --out_jsonl results/runs_llamacpp.jsonl
```

**推理过程实时输出示例：**
```
================================================================================
INITIALIZING POWER MONITOR
================================================================================
Tegrastats monitor started (interval: 100ms)
Measuring idle power for 10.0 seconds...
Idle power: 3542.3 mW (from 100 samples)

================================================================================
BENCHMARKING: qwen1.5-1.8b
================================================================================

Loading model: models/qwen1_5-1_8b-chat-q6_k.gguf
  Context size: 4096
  GPU layers: -1
  Model loaded in 3.45s

Task 1/50
  [L0] Q81 - writing
    Latency: 1234.5ms | Tokens: 22→156 | Energy: 2.345J
  [L1] Q81 - writing
    Latency: 2345.6ms | Tokens: 61→178 | Energy: 3.456J
...
```

**第2步：导出responses（方便复查）**

```bash
# 导出所有格式
python3 export_responses.py \
  --runs results/runs_llamacpp.jsonl \
  --all

# 输出：
# exported_responses/all_responses.csv          ← Excel查看
# exported_responses/responses_by_model/        ← 文本文件
# exported_responses/summary_report.txt         ← 统计摘要
```

**第3步：LLM-as-Judge评分（可在PC或Jetson，需联网）**

```bash
# API key已内置，直接运行
python3 judge_absolute.py \
  --questions question.jsonl \
  --runs results/runs_llamacpp.jsonl

# 输出：results/scores_absolute.csv
```

**评分过程实时输出示例：**
```
================================================================================
JUDGING OUTPUTS (ABSOLUTE SCORING)
================================================================================

Judging 200 outputs...
  [1/200] Judging Q81 - qwen1.5-1.8b - L0
    Score: 7.5/10 - Good response but slightly verbose
  [2/200] Judging Q81 - qwen1.5-1.8b - L1
    Score: 8.5/10 - Excellent comprehensive response
...
```

**第4步：生成Excel报告**

```bash
python3 aggregate_to_excels_absolute.py \
  --runs results/runs_llamacpp.jsonl \
  --scores results/scores_absolute.csv \
  --outdir results/

# 输出：
# results/quality_scores_detailed.xlsx      ← 详细评分
# results/model_comparison.xlsx             ← 模型对比⭐
# results/energy_per_run.xlsx               ← 能耗数据
# results/latency_per_run.xlsx              ← 延迟数据
# results/output_tokens_per_run.xlsx        ← 输出长度
```

---

## ⏱️ 时间和成本估算

### 单个类别（10个问题）

| 阶段 | 时间 | 说明 |
|------|------|------|
| 推理 | 15-30分钟 | 4模型 × 5级别 × 10问题 = 200次推理 |
| 导出 | 30秒 | 自动导出CSV和文本 |
| 评分 | 5-10分钟 | 200次API调用（GPT-4） |
| 报告 | 30秒 | 生成Excel文件 |
| **总计** | **~25分钟** | 不含模型加载 |

### 全部8个类别（80个问题）

| 阶段 | 时间 | 成本 |
|------|------|------|
| 推理 | 2-4小时 | 免费（本地） |
| 导出 | 2分钟 | 免费 |
| 评分 | 40-60分钟 | ~$8 (GPT-4o) |
| 报告 | 2分钟 | 免费 |
| **总计** | **~3-5小时** | **~$8** |

**推理时间详细计算：**
```
单次推理平均时间：5秒
总推理次数：80问题 × 5级别 × 4模型 = 1600次
理论时间：1600 × 5秒 = 8000秒 = 2.2小时
实际时间：~3小时（含模型加载、功耗测量等）
```

**API成本详细计算：**
```
评分次数：1600次（每次推理一次评分）
单次成本：~$0.005 (GPT-4o)
总成本：1600 × $0.005 = $8
```

**省钱提示：**
- 使用 `gpt-4o-mini` 成本降至 $0.8（便宜10倍）
- 使用本地LLM评分：完全免费

---

## 📁 输出文件说明

### 运行完成后的目录结构

```
results/
├── runs_llamacpp.jsonl              # 原始推理日志（1600行，每行一次推理）
├── scores_absolute.csv              # GPT-4评分结果（1600行）
├── quality_scores_detailed.xlsx     # 详细评分（按模型分sheet）⭐
├── model_comparison.xlsx            # 模型对比汇总表⭐⭐⭐
├── energy_per_run.xlsx              # 能耗数据矩阵
├── latency_per_run.xlsx             # 延迟数据矩阵
└── output_tokens_per_run.xlsx       # 输出长度矩阵

exported_responses/
├── all_responses.csv                # 所有回答的CSV（Excel查看）⭐
├── summary_report.txt               # 统计摘要报告
└── responses_by_model/              # 按模型分类的文本文件
    ├── qwen1.5-1.8b/
    │   ├── Q081_L0.txt
    │   ├── Q081_L1.txt
    │   └── ... (400个文件)
    ├── gemma-2-2b/
    ├── phi-3.5-mini/
    └── qwen3-4b/
```

### 重点查看的文件

**1. `model_comparison.xlsx`** ⭐⭐⭐ 最重要
- 各模型在不同Prompt级别的平均分数
- 快速对比哪个模型/级别最好
- 写论文的核心数据来源

**2. `all_responses.csv`** ⭐⭐
- 用Excel打开，可以筛选、搜索
- 人工复查模型回答质量
- 复制示例到报告中

**3. `quality_scores_detailed.xlsx`** ⭐⭐
- 每个问题的详细评分（4个维度）
- 包含GPT-4的评分理由
- 分析哪些问题效果好/差

---

## 🔍 检查点和验证

### 推理完成后检查

```bash
# 1. 检查JSONL行数（应该是 80×5×4=1600 或更少如果只测试部分类别）
wc -l results/runs_llamacpp.jsonl

# 2. 检查是否有4个模型的数据
grep -o '"model_id":"[^"]*"' results/runs_llamacpp.jsonl | sort | uniq -c

# 应该看到：
#  400 "model_id":"qwen1.5-1.8b"
#  400 "model_id":"gemma-2-2b"
#  400 "model_id":"phi-3.5-mini"
#  400 "model_id":"qwen3-4b"

# 3. 检查5个级别都有数据
grep -o '"level":"[^"]*"' results/runs_llamacpp.jsonl | sort | uniq -c

# 应该看到：
#  320 "level":"L0"
#  320 "level":"L1"
#  320 "level":"L2"
#  320 "level":"L3"
#  320 "level":"P"

# 4. 查看第一条记录（确认格式正确）
head -n 1 results/runs_llamacpp.jsonl | python3 -m json.tool
```

### 评分完成后检查

```bash
# 1. 检查评分CSV行数（应该等于推理次数）
wc -l results/scores_absolute.csv

# 2. 检查分数范围（应该在0-10之间）
python3 -c "
import csv
with open('results/scores_absolute.csv') as f:
    reader = csv.DictReader(f)
    scores = [float(row['total_score']) for row in reader]
    print(f'Min: {min(scores):.1f}, Max: {max(scores):.1f}, Avg: {sum(scores)/len(scores):.1f}')
"
```

---

## ⚠️ 常见问题和解决方案 （我让gpt想的可能会出现的问题，仅供参考）

### Q1: `tegrastats not found`
```bash
# 检查是否在Jetson设备上
which tegrastats
# 如果没有，说明不在Jetson上，该benchmark必须在Jetson运行
```

### Q2: Out of Memory (OOM)
```bash
# 减小context window和max_tokens
python3 run_llamacpp_collect.py \
  --n_ctx 2048 \      # 从4096降到2048
  --max_tokens 512    # 从1024降到512
```

### Q3: 推理太慢
```bash
# 1. 确认GPU加速已启用
python3 -c "from llama_cpp import Llama; m=Llama('models/qwen1_5-1_8b-chat-q6_k.gguf', n_gpu_layers=-1); print('GPU OK')"

# 2. 先测试小模型
python3 run_llamacpp_collect.py \
  --models models/qwen1_5-1_8b-chat-q6_k.gguf  # 只测最小的
```

### Q4: API rate limit
```bash
# 增加延迟
python3 judge_absolute.py \
  --sleep 1.0  # 从0.2秒增加到1.0秒
```

### Q5: 磁盘空间不足
```bash
# 检查空间
df -h

# 推理结果压缩备份
tar -czf results_backup.tar.gz results/
# 删除原始文件
rm -rf results/

# 需要至少15GB空间：
# - 模型: 8GB
# - 结果: 2-3GB
# - 导出: 1-2GB
# - 临时: 2GB
```

---

## 🎯 成功标志

运行完整流程后，你应该有：

✅ **1600条推理记录** (`runs_llamacpp.jsonl`)
✅ **1600条评分记录** (`scores_absolute.csv`)
✅ **1600个response文本文件** (`exported_responses/responses_by_model/`)
✅ **1个CSV文件** 包含所有responses (`all_responses.csv`)
✅ **6个Excel报告** (质量、能耗、延迟等)
✅ **统计摘要** 证明测试完整性

**数据完整性验证：**
- 4个模型都有数据
- 5个级别都有数据
- 80个问题都有结果
- Token数递增：L0 < L1 < L2 < L3 < P
- 分数在合理范围（5-9分）

---

## 📊 后续分析建议

1. **打开 `model_comparison.xlsx`**
   - 看哪个模型总分最高
   - 看哪个Prompt级别提升最大

2. **分析质量-成本权衡**
   - L2 vs L0：质量提升了多少？能耗增加了多少？
   - 找到最佳性价比的级别

3. **案例研究**
   - 从 `all_responses.csv` 中挑选好/坏的示例
   - 写论文时引用

4. **绘制图表**
   - 质量 vs Prompt长度（散点图）
   - 能耗 vs 质量（帕累托前沿）
   - 不同模型的雷达图

---

---

**祝测试顺利！预计3-5小时完成全部1600次推理和评分。** 🚀📊✨
